{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization for LM\n",
    "\n",
    "Reducing the size of Neural Networks is called Network Quantizaiton where the \"quantization\" meaning number of bits representing a number.\n",
    "\n",
    "**nn.Module** vs **nn.Functional**\n",
    "The technique we are going to use for quantization called \"net-aware quantizaiton\" is 1:1 mapping of operations and to model. So we cannot re-use operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from tqdm import tqdm\n",
    "from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARN)\n",
    "\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
    "   logging.WARN)  # Reduce logging\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 1\n",
      "\tat::get_num_interop_threads() : 4\n",
      "OpenMP not found\n",
      "Intel(R) Math Kernel Library Version 2019.0.5 Product Build 20190808 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 1\n",
      "Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)\n",
      "std::thread::hardware_concurrency() : 8\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(1)\n",
    "print(torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load wikitext from the torchtext module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx.keys():\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) -1\n",
    "        return self.word2idx[word]\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "    \n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "    \n",
    "    def tokenize(self, path):\n",
    "        \n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                    \n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "        return ids\n",
    "    \n",
    "model_data_filepath = '../'\n",
    "\n",
    "corpus = Corpus('../wikitext-2')\n",
    "                    \n",
    " \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTMModel:\n\tsize mismatch for encoder.weight: copying a param with shape torch.Size([33278, 512]) from checkpoint, the shape in current model is torch.Size([18328, 512]).\n\tsize mismatch for decoder.weight: copying a param with shape torch.Size([33278, 256]) from checkpoint, the shape in current model is torch.Size([18328, 256]).\n\tsize mismatch for decoder.bias: copying a param with shape torch.Size([33278]) from checkpoint, the shape in current model is torch.Size([18328]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2f3b72b2dcda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     torch.load(\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'word_language_model_quantize.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTMModel:\n\tsize mismatch for encoder.weight: copying a param with shape torch.Size([33278, 512]) from checkpoint, the shape in current model is torch.Size([18328, 512]).\n\tsize mismatch for decoder.weight: copying a param with shape torch.Size([33278, 256]) from checkpoint, the shape in current model is torch.Size([18328, 256]).\n\tsize mismatch for decoder.bias: copying a param with shape torch.Size([33278]) from checkpoint, the shape in current model is torch.Size([18328])."
     ]
    }
   ],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = LSTMModel(\n",
    "    ntoken = ntokens,\n",
    "    ninp = 512,\n",
    "    nhid = 256,\n",
    "    nlayers = 5,\n",
    ")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        'word_language_model_quantize.pth',\n",
    "        map_location=torch.device('cpu')\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(ntokens, (1,1), dtype=torch.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
